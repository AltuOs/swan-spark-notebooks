{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 20%; margin-right: 15%; margin-left: 15%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 15%; margin-bottom: 2.0em;\">\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>Integration of SWAN with Spark clusters</h1></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current setup allows to execute PySpark operations on CERN Hadoop and Spark clusters. This notebook illustrates the use of Spark in SWAN to analyze the monitoring data available on HDFS and plots a heatmap of loadAvg across machines in a particular service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to a cluster, click on the star button on the top and follow the instructions\n",
    "* The star button only appears if you have selected a SPARK cluster in the configuration\n",
    "* The star button is active after the notebook kernel is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary spark and python stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, when, col\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select the data\n",
    "*path_on_hdfs_to_your_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/project/itmon/archive/lemon/hadoop/2018-04/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aggregated: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- metric_id: string (nullable = true)\n",
      " |-- metric_name: string (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- submitter_environment: string (nullable = true)\n",
      " |-- submitter_host: string (nullable = true)\n",
      " |-- submitter_hostgroup: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- toplevel_hostgroup: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filter for loadAvg\n",
    "_https://metricmgr.cern.ch/metric/20002/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_schema = StructType([StructField(\"LoadAvg\",DoubleType())])\n",
    "df_loadAvg = df.where(col(\"metric_id\") == \"20002\").withColumn('body', from_json('body', body_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create temporary table view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# body_schema = spark.read.json(df_loadAvg.rdd.map(lambda row: row.body)).schema\n",
    "df_loadAvg.createOrReplaceTempView(\"loadAvg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the heavylifting in spark and collect aggregated view to panda DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_loadAvg_pandas = spark.sql(\"SELECT submitter_host, \\\n",
    "                                      avg(body.LoadAvg) as avg, \\\n",
    "                                      hour(from_unixtime(timestamp / 1000, 'yyyy-MM-dd HH:mm:ss')) as hr \\\n",
    "                               FROM loadAvg \\\n",
    "                               WHERE submitter_hostgroup = 'hadoop/itdb/datanode' \\\n",
    "                               AND dayofmonth(from_unixtime(timestamp / 1000, 'yyyy-MM-dd HH:mm:ss')) = 15 \\\n",
    "                               GROUP BY hour(from_unixtime(timestamp / 1000, 'yyyy-MM-dd HH:mm:ss')), submitter_host\")\\\n",
    "                    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of service availability\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.heatmap(df_loadAvg_pandas.pivot(index='submitter_host', columns='hr', values='avg'), cmap=\"Blues\")\n",
    "ax.set_title(\"Heatmap of loadAvg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.dynamicAllocation.maxExecutors",
     "value": "600"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
